
# Ex.No: 2 	Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: ChatGPT, Claude, Bard, Cohere Command, and Meta 
### DATE:12.04.2025                                                                           
### REGISTER NUMBER : 212222240118
 
# Aim:
To compare the performance, user experience, and response quality of different AI platforms (ChatGPT, Claude, Bard, Cohere Command, and Meta) within a specific use case, such as summarizing text or answering technical questions. Generate a Prompt based output using different Prompting tools of 2024.
# AI Tools required:
Deep Seek
# Explanation:
Define the Use Case:
Select a specific task for evaluation across platforms (e.g., summarizing a document, answering a technical question, or generating a creative story / Code).
Ensure the use case is applicable to all platforms and will allow for comparison across response quality, accuracy, and depth.
Create a Set of Prompts:
Prepare a uniform set of prompts that align with the chosen use case.
Each prompt should be clear and precise, ensuring that all platforms are evaluated using the same input.
Consider multiple prompts to capture the versatility of each platform in handling different aspects of the use case.
Run the Experiment on Each AI Platform:
Input the prompts into each AI tool (ChatGPT, Claude, Bard, Cohere Command, and Meta) and gather the responses.
Ensure the same conditions are applied for each platform, such as input format, time to respond, and prompt delivery.
Record response times, ease of interaction with the platform, and any technical issues encountered.
Evaluate Response Quality:
Assess each platform’s responses using the following criteria: Accuracy,Clarity,Depth,Relevance 
Compare Performance:
Compare the collected data to identify differences in performance across platforms.
Identify any platform-specific advantages, such as faster response times, more accurate answers, or more intuitive interfaces.
Deliverables:
A comparison table outlining the performance of each platform (ChatGPT, Claude, Bard, Cohere Command, and Meta) based on accuracy, clarity, depth, and relevance of responses.
A final report summarizing the findings of the experiment, including recommendations on the most suitable AI platform for different use cases based on performance and user 

# Output:

# Output & Conclusion
# Use Case:
Task: Answering technical questions about machine learning concepts.
# Sample Prompts:

“Explain the transformer architecture’s self-attention mechanism in simple terms.”

“Write a Python function to calculate the Fibonacci sequence using recursion.”

“What are the ethical risks of deploying large language models in healthcare?”

# Comparison Table
![image](https://github.com/user-attachments/assets/244a6a42-413d-4b39-b799-5a0e9cf6cf91)

# Detailed Analysis
# Accuracy:

ChatGPT provided the most technically precise answers (e.g., correctly explaining multi-head attention).

Bard occasionally oversimplified concepts, leading to minor inaccuracies.

# Clarity:

Claude delivered exceptionally structured explanations with bullet points and analogies.

Cohere Command used more technical jargon, reducing accessibility for non-experts.

# Depth:

Claude and ChatGPT included real-world examples (e.g., transformer applications in BERT).

Meta AI provided shorter answers with less contextualization.

# Relevance:

ChatGPT and Cohere Command stayed strictly on-topic.

Bard occasionally added tangential details (e.g., mentioning RNNs in a transformer explanation).

# Response Time:

Bard was fastest, but sacrificed depth for speed.

Meta AI lagged due to computational overhead.

# User Experience:

ChatGPT and Claude offered intuitive interfaces with easy prompt editing.

Cohere Command required more precise prompting to avoid generic outputs.

# Sample Prompt Output
![image](https://github.com/user-attachments/assets/85860ae8-9254-43eb-8d91-8fa00451b65a)

# ChatGPT:
“Self-attention allows the model to weigh the importance of different words in a sentence. For example, in ‘The cat sat on the mat,’ it identifies ‘cat’ and ‘mat’ as key elements by assigning higher attention scores to them.”

# Claude:
“Imagine reading a sentence and highlighting the most important words. Self-attention does this computationally, letting the model focus on relevant parts (like ‘cat’ and ‘mat’) to understand context.”

# Bard:
“Self-attention is like a spotlight that helps the model focus on specific words to generate accurate predictions.”

# Conclusion: 

Best Overall: ChatGPT (GPT-4) excelled in accuracy, speed, and user experience, making it ideal for technical tasks requiring precision.

Depth & Clarity: Claude is recommended for educational or explanatory use cases due to its structured, analogy-driven responses.

Speed vs. Depth: Bard suits quick, high-level summaries but lags in technical depth.

Niche Use: Cohere Command performs well in domain-specific tasks (e.g., healthcare or legal QA) with tailored prompts.

Open-Source Option: Meta AI (Llama 3) is viable for developers prioritizing customization over speed.

# Result :
The Prompt for the above problem statement executed successfully.
